{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing a chatbot with Retrieval-Augmented Generation (RAG) using LLama 2 Chat (13B), vLLM, LangChain, and the Gradio UI.\n",
    "- [**Langchain**](https://python.langchain.com/docs/get_started/introduction) is a framework for the fast prototyping of applications powered by language models. In this example we replaced the Gradio's chatbot memory with LangChain's memory implemented by the `LLMChain` class.\n",
    "- [**vLLM**](https://www.gradio.app/) is a fast and easy-to-use library for LLM inference and serving. In this example, we use LangChain's `VLLMOpenAI` class to access the vLLM OpenAI API-compatible service.\n",
    "- [**Gradio**](https://www.gradio.app/) is a fast way to demo machine learning models with a friendly web interface.\n",
    "\n",
    "### Introduction\n",
    "Retrieval-Augmented Generation (RAG) systems combine the capabilities of large language models and information retrieval systems to improve the quality of generated responses. In a RAG system, the process generally involves two main steps:\n",
    "\n",
    "1. **Retrieval**: Given a query, the system retrieves a set of relevant documents or passages from a pre-existing knowledge base or corpus. This retrieval step is usually performed using traditional information retrieval techniques or learned embeddings.\n",
    "\n",
    "2. **Generation**: The retrieved documents are then used to augment the input to a generative model, like Llama 2. The model uses this additional context to generate more informed, relevant, and accurate responses.\n",
    "\n",
    "By combining retrieval and generation, RAG systems aim to provide answers that are not only contextually coherent but also factually accurate, even when the information is not explicitly present in the model's pre-training data.\n",
    "\n",
    "**In this RAG example, we expand the LLama 2 knowledge with a knowledge base consisting of three NY Times articles (serialized as PDF documents) about the Otis category 5 hurricane which stormed the Pacific coast of Mexico by the end of October 2023. This knowledge base contains data that is posterior to the datasets used to pre-train LLama 2 and fine-tune Llama 2 Chat, which date prior July 2023.** \n",
    "\n",
    "### Requirements\n",
    "Please follow the instructions provided by the README.md file from the \n",
    "[LLM-fine-tuning-example](https://github.com/vmware-ai-labs/VMware-generative-ai-reference-architecture/tree/main/Examples/LLM-fine-tuning-example) directory to create a conda virtual environment with all the libraries used in this notebook.\n",
    "\n",
    "To use Llama2 from the Transformers library you need to get approval from Meta and HuggingFace. \n",
    "- Please follow these [instructions provided by HuggingFace](https://huggingface.co/meta-llama) before continuing.\n",
    "- Then [create a Hugging Face user access token](https://huggingface.co/docs/hub/security-tokens) to be able to download LLama 2 models from HF Hub.\n",
    "- Once you have created an access token, please login from the shell terminal using any of these options:\n",
    "    ```\n",
    "    # Using an interactive command\n",
    "    huggingface-cli login\n",
    "  \n",
    "    # or using an environment variable\n",
    "    huggingface-cli login --token $HUGGINGFACE_TOKEN\n",
    "    ```\n",
    "- You can serve Llama-2-13B-chat on an A100 (40GB) GPU using vLLM by running the following command:\n",
    "`python -m vllm.entrypoints.openai.api_server --model=\"meta-llama/Llama-2-13b-chat-hf\"`\n",
    "\n",
    "Now, you should be all set to get started!\n",
    "## Imports section"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bce361dd40038f4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.llms import VLLMOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a59248939ae159e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generation parameters initialization and helper functions definitions\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edc63ede0269429a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "# URL where vLLM serves the Llama-2-13b-chat LLM\n",
    "INFERENCE_SRV_URL = \"http://localhost:8000/v1\"\n",
    "\n",
    "def setup_chat_llm(max_tokens=500, temperature=0.1, top_p=.9):\n",
    "    \"\"\"\n",
    "    Initializes the llm chat object that allows language chains get access\n",
    "    to the Llama 2 LLM service.\n",
    "    :param max_tokens: Max num. of tokens to generate\n",
    "    :param max_tokens: Max num. of tokens to generate.\n",
    "    :param temperature:  Determines how creative the model should be.\n",
    "    :param top_p: Determines how creative the model should be.\n",
    "    :return: the llm service callable object\n",
    "    \"\"\"\n",
    "    llm = VLLMOpenAI(\n",
    "        openai_api_key = \"EMPTY\",\n",
    "        openai_api_base = INFERENCE_SRV_URL,\n",
    "        model_name = \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temperature,\n",
    "        top_p = top_p,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "# Initialize LLM service\n",
    "llm = setup_chat_llm()\n",
    "\n",
    "# >>> Set up the embedding encoder (Sentence Transformers)\n",
    "# Notice that CPUs can be use to encode text from knowledge bases.\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# >>> Load all PDF documents containing the NY Times articles\n",
    "# about hurricane Otis from the \"files\" directory\n",
    "loader = PyPDFDirectoryLoader(path=\"./files\")\n",
    "\n",
    "# Split the documents into chunks.\n",
    "doc_splits = loader.load_and_split()\n",
    "\n",
    "# >>> Encode the document splits using the embeddings encoder.\n",
    "# The encoded splits get stored into a Chroma vector database\n",
    "# which serves as a retriever of chunks of text related to a user's query.\n",
    "retriever = Chroma.from_documents(documents=doc_splits, embedding=embeddings).as_retriever()\n",
    "\n",
    "# >>> Define a LLama RAG prompt that instructs the LLM to generate answers to users' queries\n",
    "# using the context provided by the retriever and the chat history (kept in the chain's memory).\n",
    "prompt_template = (\n",
    "\"\"\"[INST]<<SYS>> \n",
    "You are an assistant for question-answering tasks. \n",
    "If you don't know the answer, just say that you don't know. Keep the answer concise.\n",
    "Use the following context delimited by <CTX></CTX>, and the chat history delimited by <HS></HS> to answer the question.<</SYS>>  \n",
    "\n",
    "<CTX>\n",
    "{context} \n",
    "</CTX>\n",
    "\n",
    "<HS>\n",
    "{history}\n",
    "</HS>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: [/INST]\"\"\")\n",
    "\n",
    "# >>> Create the prompt template object including the explicit declaration of\n",
    "# input variables that get dynamically entered as a conversation evolves.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Initialize the memory object that tracks a user's conversation with the LLM\n",
    "memory = ConversationBufferMemory(memory_key=\"history\",\n",
    "                                  input_key=\"question\",\n",
    "                                  return_messages=True)\n",
    "\n",
    "# >>> RetrievalQA chain initialization. This object coordinates actions among the LLM service, the context retriever\n",
    "# and the conversation history (memory) to generate responses to users' queries about a knowledge base. \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"prompt\": prompt,\n",
    "        \"memory\": memory,\n",
    "    }\n",
    ")\n",
    "\n",
    "def respond(user_message, chatbot, conversation=qa_chain, mem=memory):\n",
    "    \"\"\"\n",
    "    Generates a completion for a new prompt using the context provided by the \n",
    "    RetrievalQA's memory and the conversation history.\n",
    "    :param user_message: New message from user properly formatted for Llama 2\n",
    "    :param conversation: The RetrievalQA object that generates responses\n",
    "    :param chatbot: Gradio's chatbot memory. Not needed by LangChain\n",
    "    :param mem: RetrievalQA's memory. \n",
    "    :return: The response to be returned to the chatbot UI\n",
    "    \"\"\"\n",
    "    # >>> Clear memory in the event of a reset button press\n",
    "    # used as workaround as it isn't possible to do from the RetrievalQA object.\n",
    "    if not len(chatbot):\n",
    "        mem.clear()\n",
    "    # Run the RetrievalQA chain \n",
    "    response = conversation.run(user_message)\n",
    "    # Store user's message and the LLM response in chatbot history\n",
    "    chatbot.append((user_message, response))\n",
    "    # Return the expanded dialog history\n",
    "    return \"\", chatbot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e103af0eccd67f79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up the Gradio client session\n",
    "with gr.Blocks() as demo:\n",
    "    # Set the size of the chat window\n",
    "    chatbot = gr.Chatbot(height=500, show_copy_button=True, interactive=True)\n",
    "    # Set the text box for users to enter text.\n",
    "    msg = gr.Textbox(label=\"Prompt\", interactive=True, show_copy_button=True)\n",
    "    # Expose 'submit' button\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    # Expose \"clear console\" button\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], \n",
    "                           value=\"Clear chat\")\n",
    "    # Submit the prompt completion request on a click event\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    # Submit the prompt completion request on an Enter key press event\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "# Close pre-existing gradio clients\n",
    "gr.close_all()\n",
    "\n",
    "# Launch Gradio demo on a new browser tab\n",
    "demo.launch(\n",
    "    inline=False,\n",
    "    inbrowser=True,\n",
    "    share=False,\n",
    "    show_error=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59b6d441cfb8db98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test\n",
    "You can try the following dialog with the chatbot by introducing the prompts shown after `Human:` using the UI launched in your web browser. <br><br>\n",
    "\n",
    "### Example\n",
    "Human: <blockquote>Where and when hurricane Otis made landfall?</blockquote>\n",
    "AI: <blockquote>Hurricane Otis made landfall near the coastal city of Acapulco in southern Mexico on Wednesday, October 25, 2023, around 1 a.m. local time.</blockquote>\n",
    "\n",
    "Human: <blockquote>What is hurricane rapid intensification?</blockquote>\n",
    "AI: <blockquote>Sure, I'd be happy to help! Hurricane rapid intensification refers to a situation where a hurricane's wind speed increases by at least 35 miles per hour within a 24-hour period. This is considered an extreme event, and it is difficult to predict with certainty. In the case of Hurricane Otis, it rapidly intensified from a tropical storm to a Category 5 hurricane in less than a day, with wind speeds increasing by 115 miles per hour in just 24 hours.</blockquote>\n",
    "\n",
    "Human: <blockquote>Summarize your previous responses using a single sentence.</blockquote>\n",
    "AI: <blockquote>AI: Sure! Here's a summary of my previous responses:\n",
    "\n",
    "Hurricane Otis made landfall in Mexico, specifically in the states of Guerrero and Oaxaca, on Wednesday, October 25, 2023, at 1 a.m. local time, and it rapidly intensified from a tropical storm to a Category 5 hurricane in less than a day, with wind speeds increasing by 115 miles per hour in just 24 hours.</blockquote>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "150b245ff59fcb9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fd01fd79ba5af9b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
